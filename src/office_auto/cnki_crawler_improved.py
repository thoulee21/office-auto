"""
çŸ¥ç½‘è®ºæ–‡çˆ¬è™«æ¨¡å— - æ”¹è¿›ç‰ˆ
æ ¹æ®ä½œè€…å§“åå’Œå•ä½çˆ¬å–è®ºæ–‡ä¿¡æ¯å¹¶ä¿å­˜åˆ°Excel
è§£å†³äº†é¡µé¢å…ƒç´ å®šä½é—®é¢˜ï¼Œå¢åŠ äº†å¤šç§æœç´¢ç­–ç•¥
"""

import re
import time
from typing import Dict, List, Optional

import pandas as pd
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager


class CNKICrawlerImproved:
    """çŸ¥ç½‘è®ºæ–‡çˆ¬è™«ç±» - æ”¹è¿›ç‰ˆ"""

    def __init__(self, headless: bool = True, wait_time: int = 15):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            wait_time: é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´
        """
        self.base_url = "https://kns.cnki.net"
        self.search_url = "https://kns.cnki.net/kns8s/"  # æ›´æ–°URL
        self.wait_time = wait_time
        self.driver = None
        self.setup_driver(headless)

    def setup_driver(self, headless: bool = True):
        """è®¾ç½®Chromeé©±åŠ¨"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")

        # æ·»åŠ æ›´å¤šç¨³å®šæ€§é€‰é¡¹
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # è‡ªåŠ¨ä¸‹è½½å¹¶è®¾ç½®Chromeé©±åŠ¨
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, self.wait_time)

    def search_papers(
        self, author_name: str, institution: str = "", max_pages: int = 5
    ) -> List[Dict]:
        """
        æœç´¢è®ºæ–‡

        Args:
            author_name: ä½œè€…å§“å
            institution: ä½œè€…å•ä½
            max_pages: æœ€å¤§æœç´¢é¡µæ•°

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        papers = []

        try:
            print(f"æ­£åœ¨æœç´¢ä½œè€…: {author_name}, å•ä½: {institution}")

            # æ–¹æ³•1ï¼šå°è¯•ç›´æ¥æœç´¢URLæ„é€ 
            success = self._try_direct_search(author_name, institution)

            if not success:
                # æ–¹æ³•2ï¼šå°è¯•è®¿é—®æœç´¢é¡µé¢å¡«å†™è¡¨å•
                success = self._try_form_search(author_name, institution)

            if success:
                # çˆ¬å–æœç´¢ç»“æœ
                papers = self._crawl_search_results(max_pages)
            else:
                print("âŒ æ— æ³•å®Œæˆæœç´¢ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–çŸ¥ç½‘å¯è®¿é—®æ€§")

        except Exception as e:
            print(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

        return papers

    def _try_direct_search(self, author_name: str, institution: str = "") -> bool:
        """å°è¯•é€šè¿‡ç›´æ¥æ„é€ æœç´¢URLè¿›è¡Œæœç´¢"""
        try:
            print("å°è¯•ç›´æ¥æœç´¢æ–¹å¼...")

            # æ„é€ æœç´¢å‚æ•°
            search_query = f"ä½œè€…:{author_name}"
            if institution:
                search_query += f" AND å•ä½:{institution}"

            # æ„é€ æœç´¢URLï¼ˆåŸºäºçŸ¥ç½‘çš„æœç´¢å‚æ•°ï¼‰
            search_url = f"{self.base_url}/kns8s/search?crossref=N&kw={search_query}"

            self.driver.get(search_url)
            time.sleep(5)

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¿›å…¥æœç´¢ç»“æœé¡µé¢
            try:
                self.wait.until(
                    EC.presence_of_element_located(
                        (
                            By.CSS_SELECTOR,
                            ".result-table-list, .searchResult, .search-result",
                        )
                    )
                )
                print("âœ… ç›´æ¥æœç´¢æˆåŠŸ")
                return True
            except TimeoutException:
                print("ç›´æ¥æœç´¢æœªæˆåŠŸï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
                return False

        except Exception as e:
            print(f"ç›´æ¥æœç´¢æ–¹å¼å¤±è´¥: {str(e)}")
            return False

    def _try_form_search(self, author_name: str, institution: str = "") -> bool:
        """å°è¯•é€šè¿‡å¡«å†™æœç´¢è¡¨å•è¿›è¡Œæœç´¢"""
        try:
            print("å°è¯•è¡¨å•æœç´¢æ–¹å¼...")

            # è®¿é—®çŸ¥ç½‘ä¸»é¡µ
            self.driver.get(self.base_url)
            time.sleep(3)

            # å°è¯•å¤šç§æœç´¢æ¡†å®šä½ç­–ç•¥
            search_input = None
            search_selectors = [
                "input[placeholder*='è¯·è¾“å…¥æ£€ç´¢è¯']",
                "input[placeholder*='æ£€ç´¢']",
                ".search-input input",
                "#searchText",
                ".nav-search input",
            ]

            for selector in search_selectors:
                try:
                    search_input = self.driver.find_element(By.CSS_SELECTOR, selector)
                    break
                except Exception as e:
                    print(f"æŸ¥æ‰¾æœç´¢è¾“å…¥æ¡†æ—¶å‡ºé”™: {str(e)}")
                    continue

            if not search_input:
                print("æœªæ‰¾åˆ°æœç´¢è¾“å…¥æ¡†")
                return False

            # æ„é€ æœç´¢æŸ¥è¯¢
            search_query = f"ä½œè€…:{author_name}"
            if institution:
                search_query += f" AND å•ä½:{institution}"

            # è¾“å…¥æœç´¢æ¡ä»¶
            search_input.clear()
            search_input.send_keys(search_query)
            search_input.send_keys(Keys.RETURN)

            print(f"å·²è¾“å…¥æœç´¢æ¡ä»¶: {search_query}")
            time.sleep(5)

            # æ£€æŸ¥æœç´¢ç»“æœ
            try:
                self.wait.until(
                    EC.presence_of_element_located(
                        (
                            By.CSS_SELECTOR,
                            ".result-table-list, .searchResult, .search-result",
                        )
                    )
                )
                print("âœ… è¡¨å•æœç´¢æˆåŠŸ")
                return True
            except TimeoutException:
                print("è¡¨å•æœç´¢æœªè¿”å›ç»“æœ")
                return False

        except Exception as e:
            print(f"è¡¨å•æœç´¢æ–¹å¼å¤±è´¥: {str(e)}")
            return False

    def _crawl_search_results(self, max_pages: int = 5) -> List[Dict]:
        """çˆ¬å–æœç´¢ç»“æœ"""
        papers = []
        current_page = 1

        while current_page <= max_pages:
            try:
                print(f"æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ...")

                # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
                time.sleep(3)

                # è·å–å½“å‰é¡µé¢çš„è®ºæ–‡åˆ—è¡¨
                page_papers = self._extract_papers_from_page()

                if not page_papers:
                    print(f"ç¬¬ {current_page} é¡µæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®")
                    if current_page == 1:
                        print("ç¬¬ä¸€é¡µå°±æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½æœç´¢æ¡ä»¶æœ‰è¯¯æˆ–ç½‘ç«™ç»“æ„å˜åŒ–")
                    break

                papers.extend(page_papers)
                print(f"ç¬¬ {current_page} é¡µè·å–åˆ° {len(page_papers)} ç¯‡è®ºæ–‡")

                # å°è¯•ç¿»åˆ°ä¸‹ä¸€é¡µ
                if not self._go_to_next_page():
                    print("æ²¡æœ‰æ›´å¤šé¡µé¢")
                    break

                current_page += 1
                time.sleep(2)

            except Exception as e:
                print(f"çˆ¬å–ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {str(e)}")
                break

        return papers

    def _extract_papers_from_page(self) -> List[Dict]:
        """ä»å½“å‰é¡µé¢æå–è®ºæ–‡ä¿¡æ¯"""
        papers = []

        try:
            # å°è¯•å¤šç§ç»“æœåˆ—è¡¨é€‰æ‹©å™¨
            paper_items = []
            result_selectors = [
                ".result-table-list tr:not(:first-child)",  # ä¼ ç»Ÿè¡¨æ ¼å½¢å¼
                ".searchResult .result-item",  # æ–°ç‰ˆç»“æœé¡¹
                ".search-result .item",  # å¦ä¸€ç§ç»“æœé¡¹
                ".literature-item",  # æ–‡çŒ®é¡¹
                "[data-index]",  # å¸¦ç´¢å¼•çš„é¡¹ç›®
            ]

            for selector in result_selectors:
                try:
                    paper_items = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if paper_items:
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(paper_items)} ä¸ªè®ºæ–‡é¡¹: {selector}")
                        break
                except Exception:
                    continue

            if not paper_items:
                print("æœªæ‰¾åˆ°è®ºæ–‡åˆ—è¡¨é¡¹")
                return papers

            for i, item in enumerate(paper_items):
                try:
                    paper_info = self._extract_paper_info(item)
                    if paper_info and paper_info.get("æ ‡é¢˜"):  # ç¡®ä¿æœ‰æ ‡é¢˜
                        papers.append(paper_info)
                    elif i < 3:  # åªå¯¹å‰3ä¸ªé¡¹ç›®æ‰“å°è°ƒè¯•ä¿¡æ¯
                        print(f"ç¬¬ {i + 1} ä¸ªé¡¹ç›®æœªèƒ½æå–åˆ°æœ‰æ•ˆä¿¡æ¯")
                except Exception as e:
                    if i < 3:  # åªå¯¹å‰3ä¸ªé¡¹ç›®æ‰“å°é”™è¯¯ä¿¡æ¯
                        print(f"æå–ç¬¬ {i + 1} ç¯‡è®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                    continue

        except Exception as e:
            print(f"æå–é¡µé¢è®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

        return papers

    def _extract_paper_info(self, item_element) -> Optional[Dict]:
        """æå–å•ç¯‡è®ºæ–‡çš„ä¿¡æ¯"""
        try:
            # è®ºæ–‡æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
            title = ""
            title_selectors = [
                "a.fz14",
                ".title a",
                ".literature-title a",
                "h3 a",
                "a[href*='detail']",
                ".result-item-title a",
            ]

            for selector in title_selectors:
                try:
                    title_element = item_element.find_element(By.CSS_SELECTOR, selector)
                    title = title_element.text.strip()
                    if title:
                        break
                except Exception:
                    continue

            if not title:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•ç›´æ¥æ‰¾æ–‡æœ¬
                try:
                    title = item_element.find_element(
                        By.CSS_SELECTOR, ".title, h3, .literature-title"
                    ).text.strip()
                except Exception:
                    pass

            # ä½œè€…ä¿¡æ¯
            authors = ""
            author_selectors = [
                "a[href*='author']",
                ".author a",
                ".literature-author a",
                "[data-author] a",
            ]

            for selector in author_selectors:
                try:
                    author_elements = item_element.find_elements(
                        By.CSS_SELECTOR, selector
                    )
                    if author_elements:
                        authors = "; ".join(
                            [
                                author.text.strip()
                                for author in author_elements
                                if author.text.strip()
                            ]
                        )
                        break
                except Exception:
                    continue

            # æœŸåˆŠä¿¡æ¯
            journal = ""
            journal_selectors = [
                "a[href*='journal']",
                "a[href*='magazine']",
                ".journal a",
                ".source a",
                ".literature-source a",
            ]

            for selector in journal_selectors:
                try:
                    journal_element = item_element.find_element(
                        By.CSS_SELECTOR, selector
                    )
                    journal = journal_element.text.strip()
                    if journal:
                        break
                except Exception:
                    continue

            # å‘è¡¨æ—¥æœŸ
            date = ""
            try:
                text_content = item_element.text
                date_patterns = [
                    r"(\d{4}-\d{2}-\d{2})",
                    r"(\d{4}/\d{2}/\d{2})",
                    r"(\d{4}\.\d{2}\.\d{2})",
                    r"(\d{4}å¹´\d{1,2}æœˆ)",
                    r"(\d{4}å¹´)",
                ]

                for pattern in date_patterns:
                    date_match = re.search(pattern, text_content)
                    if date_match:
                        date = date_match.group(1)
                        break
            except Exception:
                pass

            # è¢«å¼•æ¬¡æ•°
            citations = ""
            try:
                citation_selectors = [
                    "*[class*='cite']",
                    "*[class*='å¼•']",
                    ".citation-count",
                ]

                for selector in citation_selectors:
                    try:
                        citation_element = item_element.find_element(
                            By.CSS_SELECTOR, selector
                        )
                        citation_text = citation_element.text.strip()
                        if "å¼•" in citation_text or "cite" in citation_text.lower():
                            citations = citation_text
                            break
                    except Exception:
                        continue
            except Exception:
                pass

            # ä¸‹è½½æ¬¡æ•°
            downloads = ""
            try:
                download_selectors = ["*[class*='download']", "*[class*='ä¸‹è½½']"]

                for selector in download_selectors:
                    try:
                        download_element = item_element.find_element(
                            By.CSS_SELECTOR, selector
                        )
                        download_text = download_element.text.strip()
                        if (
                            "ä¸‹è½½" in download_text
                            or "download" in download_text.lower()
                        ):
                            downloads = download_text
                            break
                    except Exception:
                        continue
            except Exception:
                pass

            # åªæœ‰æ ‡é¢˜ä¸ä¸ºç©ºæ‰è¿”å›ç»“æœ
            if title:
                return {
                    "æ ‡é¢˜": title,
                    "ä½œè€…": authors,
                    "æœŸåˆŠ": journal,
                    "å‘è¡¨æ—¥æœŸ": date,
                    "è¢«å¼•æ¬¡æ•°": citations,
                    "ä¸‹è½½æ¬¡æ•°": downloads,
                }
            else:
                return None

        except Exception as e:
            print(f"æå–è®ºæ–‡è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            return None

    def _go_to_next_page(self) -> bool:
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # å°è¯•å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
            next_selectors = [
                "a[title*='ä¸‹é¡µ']",
                "a[title*='ä¸‹ä¸€é¡µ']",
                ".next-page",
                ".page-next",
                "a:contains('ä¸‹é¡µ')",
                "a:contains('ä¸‹ä¸€é¡µ')",
                "a:contains('>')",
            ]

            for selector in next_selectors:
                try:
                    next_button = self.driver.find_element(By.CSS_SELECTOR, selector)

                    # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç‚¹å‡»
                    if "disabled" in next_button.get_attribute(
                        "class"
                    ) or next_button.get_attribute("disabled"):
                        continue

                    # å°è¯•ç‚¹å‡»
                    self.driver.execute_script("arguments[0].click();", next_button)
                    time.sleep(3)
                    return True

                except Exception:
                    continue

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå°è¯•é¡µç é“¾æ¥
            try:
                current_page_text = self.driver.find_element(
                    By.CSS_SELECTOR, ".current-page, .active"
                ).text
                next_page_num = int(current_page_text) + 1
                next_page_link = self.driver.find_element(
                    By.LINK_TEXT, str(next_page_num)
                )
                next_page_link.click()
                time.sleep(3)
                return True
            except Exception:
                pass

            return False

        except Exception as e:
            print(f"ç¿»é¡µæ—¶å‡ºé”™: {str(e)}")
            return False

    def save_to_excel(self, papers: List[Dict], filename: str = "cnki_papers.xlsx"):
        """ä¿å­˜è®ºæ–‡ä¿¡æ¯åˆ°Excelæ–‡ä»¶"""
        if not papers:
            print("æ²¡æœ‰è®ºæ–‡æ•°æ®å¯ä¿å­˜")
            return

        try:
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(papers)

            # è°ƒæ•´åˆ—é¡ºåº
            columns_order = ["æ ‡é¢˜", "ä½œè€…", "æœŸåˆŠ", "å‘è¡¨æ—¥æœŸ", "è¢«å¼•æ¬¡æ•°", "ä¸‹è½½æ¬¡æ•°"]
            existing_columns = [col for col in columns_order if col in df.columns]
            df = df[existing_columns]

            # ä¿å­˜åˆ°Excel
            df.to_excel(filename, index=False, engine="openpyxl")
            print(f"âœ… æˆåŠŸä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯åˆ° {filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # ä½¿ç”¨ç¤ºä¾‹
    author_name = "é™ˆæ™¨"  # è¦æœç´¢çš„ä½œè€…å§“å
    institution = ""  # ä½œè€…å•ä½ï¼ˆå¯é€‰ï¼‰

    with CNKICrawlerImproved(headless=False) as crawler:
        # æœç´¢è®ºæ–‡
        papers = crawler.search_papers(author_name, institution, max_pages=3)

        # ä¿å­˜åˆ°Excel
        if papers:
            filename = f"{author_name}_papers.xlsx"
            crawler.save_to_excel(papers, filename)
            print(f"\nğŸ‰ æœç´¢å®Œæˆï¼æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

            # æ˜¾ç¤ºå‰3ç¯‡é¢„è§ˆ
            print("\nğŸ“‹ è®ºæ–‡é¢„è§ˆï¼š")
            for i, paper in enumerate(papers[:3], 1):
                print(f"{i}. {paper.get('æ ‡é¢˜', 'N/A')}")
                print(f"   ä½œè€…ï¼š{paper.get('ä½œè€…', 'N/A')}")
                print(f"   æœŸåˆŠï¼š{paper.get('æœŸåˆŠ', 'N/A')}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")


if __name__ == "__main__":
    main()
